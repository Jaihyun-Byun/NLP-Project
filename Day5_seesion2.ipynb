{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaihyun-Byun/NLP-Project/blob/main/Day5_seesion2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49e4b829"
      },
      "source": [
        "# ğŸ“ Comprehensive Agentic NLP Tutorial: From LLMs to Agents\n",
        "\n",
        "ì•ˆë…•í•˜ì„¸ìš”! ì´ íŠœí† ë¦¬ì–¼ì€ **ë‹¨ìˆœí•œ ì±—ë´‡(LLM)ì„ ë„˜ì–´, ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ê³  ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©° ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” 'AI ì—ì´ì „íŠ¸'**ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“š ë¬´ì—‡ì„ ë°°ìš°ë‚˜ìš”?\n",
        "ì´ ë…¸íŠ¸ë¶ í•˜ë‚˜ë¡œ ë‹¤ìŒì˜ í•µì‹¬ ê¸°ìˆ ë“¤ì„ ë§ˆìŠ¤í„°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "1.  **Code Generation**: LLMì´ ì½”ë“œë¥¼ ì§œê³ , *ìŠ¤ìŠ¤ë¡œ ì—ëŸ¬ë¥¼ ê³ ì¹˜ëŠ”* ë°©ë²•\n",
        "2.  **Advanced RAG**: ë‹¨ìˆœ ê²€ìƒ‰ì„ ë„˜ì–´, *ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°œë¡œ ìª¼ê°œì–´* ë” ì •í™•í•˜ê²Œ ê²€ìƒ‰í•˜ëŠ” ë²•\n",
        "3.  **Toos & MCP**: AIì—ê²Œ ê³„ì‚°ê¸°ë‚˜ ê²€ìƒ‰ ì—”ì§„ ê°™ì€ *'ì†ê³¼ ë°œ'*ì„ ë‹¬ì•„ì£¼ëŠ” í‘œì¤€ ê·œê²©(MCP)\n",
        "4.  **Multi-Agent Systems**: í˜¼ìì„œ ëª»í•˜ëŠ” ì¼ì„ *'íŒ€(Team)'*ì„ ê¾¸ë ¤ í•´ê²°í•˜ëŠ” ë‹¤ì–‘í•œ íŒ¨í„´ (ReAct, Reflexion ë“±)\n",
        "5.  **Advanced Features**: AIì—ê²Œ *ê¸°ì–µë ¥(Memory)*ì„ ì£¼ê³ , *ì‚¬ëŒì˜ ê²°ì¬(Human-in-the-loop)*ë¥¼ ë°›ê³ , *ì„±ëŠ¥ì„ ì±„ì (Evaluation)*í•˜ëŠ” ë²•\n",
        "\n",
        "---\n",
        "## âš ï¸ ì¤€ë¹„ë¬¼\n",
        "- **Google Colab (T4 GPU ì´ìƒ ê¶Œì¥)** ë˜ëŠ” ë¡œì»¬ í™˜ê²½\n",
        "- **Ollama**: ë¡œì»¬ LLM ì„œë²„ê°€ ì¼œì ¸ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. (ì‹¤ìŠµ í™˜ê²½ì— ë”°ë¼ ì„¤ì • í•„ìš”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64d3777c"
      },
      "source": [
        "## 1. Environment Setup (í™˜ê²½ ì„¤ì •)\n",
        "\n",
        "ìš”ë¦¬ë¥¼ í•˜ë ¤ë©´ ë„êµ¬ë‘ ì¬ë£Œê°€ í•„ìš”í•˜ì£ ? AI ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•˜ê³ , ìš°ë¦¬ì˜ ë‘ë‡Œê°€ ë  **Ollama(LLM)**ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!nohup ollama serve > ollama.log &"
      ],
      "metadata": {
        "id": "cFda37S9EXF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3bce1eb",
        "outputId": "0b8cbe2a-d052-4f24-cf8b-d39447269997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ],
      "source": [
        "# LangChain ë° MCP ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install --quiet -U openai ollama \\\n",
        "  langchain langchain_community langchain_core langchain_openai langchainhub langchain_elasticsearch langchain_ollama \\\n",
        "  transformers datasets python-dotenv tenacity google-search-results unstructured arxiv pymupdf tiktoken \\\n",
        "  streamlit streamlit-folium wikipedia jq ragas dspy \\\n",
        "  mcp langchain-mcp-adapters langgraph langchain_tavily langchain_experimental \\\n",
        "  langchain-chroma langchain-community google-search-results pymysql"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKfL49wwUtF1",
        "outputId": "9efa582a-30f7-4d89-85f1-180e21c7aa69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb  4 11:05:04 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ddf981",
        "outputId": "f2b8e60b-27f2-4b05-a75f-bf0b031a9ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "ì¤€ë¹„ ì™„ë£Œ! ëŒ€í™”ìš© ëª¨ë¸: llama3.1:8b, ì½”ë”©ìš© ëª¨ë¸: qwen2.5-coder:7b, ì„ë² ë”© ëª¨ë¸: nomic-embed-text\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ë³¸ ì„¤ì •\n",
        "import os\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ URL\n",
        "BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "# ğŸ’¡ ëª¨ë¸ ì„ íƒ\n",
        "# - llama3.1: ì¼ë°˜ì ì¸ ëŒ€í™”ì™€ ì¶”ë¡ ì„ ì˜í•¨ (ë¬¸ê³¼ìƒ ëŠë‚Œ)\n",
        "# - qwen2.5-coder: ì½”ë”©ì„ ê¸°ê°€ ë§‰íˆê²Œ ì˜í•¨ (ì´ê³¼ìƒ ëŠë‚Œ)\n",
        "LLM_NAME = \"llama3.1:8b\"\n",
        "CODE_LLM_NAME = \"qwen2.5-coder:7b\"\n",
        "EMBEDDING_NAME = \"nomic-embed-text\"\n",
        "\n",
        "\n",
        "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰ í•„ìš”)\n",
        "!ollama pull $LLM_NAME\n",
        "!ollama pull $CODE_LLM_NAME\n",
        "!ollama pull $EMBEDDING_NAME\n",
        "\n",
        "# LangChain LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "llm = ChatOllama(\n",
        "    model=LLM_NAME,\n",
        "    temperature=0,\n",
        "    base_url=BASE_URL\n",
        ")\n",
        "\n",
        "code_llm = ChatOllama(\n",
        "    model=CODE_LLM_NAME,\n",
        "    temperature=0.1,\n",
        "    base_url=BASE_URL\n",
        ")\n",
        "\n",
        "embedding = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=BASE_URL)\n",
        "\n",
        "print(f\"ì¤€ë¹„ ì™„ë£Œ! ëŒ€í™”ìš© ëª¨ë¸: {LLM_NAME}, ì½”ë”©ìš© ëª¨ë¸: {CODE_LLM_NAME}, ì„ë² ë”© ëª¨ë¸: {EMBEDDING_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeed1348"
      },
      "outputs": [],
      "source": [
        "# Helper function: ë‹¤ì´ì–´ê·¸ë¨(Mermaid)ì„ ë…¸íŠ¸ë¶ì—ì„œ ë°”ë¡œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜\n",
        "import base64\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def mm(graph):\n",
        "    graphbytes = graph.encode(\"utf-8\")\n",
        "    base64_bytes = base64.b64encode(graphbytes)\n",
        "    base64_string = base64_bytes.decode(\"ascii\")\n",
        "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a41d8d4"
      },
      "source": [
        "## 2. Code Generation (ì½”ë“œ ìƒì„± ë° ìê°€ ìˆ˜ì •)\n",
        "\n",
        "### ğŸ£ ê°œë… ì„¤ëª…: \"í•œ ë²ˆì— ì™„ë²½í•œ ì½”ë“œëŠ” ì—†ë‹¤\"\n",
        "ì½”ë”© ê³¼ì œë¥¼ í•  ë•Œ, ì½”ë“œë¥¼ í•œ ë²ˆ ì§œê³  ë°”ë¡œ ì œì¶œí•˜ë‚˜ìš”? ì•„ë‹ˆì£ .\n",
        "**ì‹¤í–‰í•´ë³´ê³ (Run) -> ì—ëŸ¬ê°€ ë‚˜ë©´(Error) -> ê³ ì¹©ë‹ˆë‹¤(Debug)**.\n",
        "\n",
        "AIë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ \"ì½”ë“œ ì§œì¤˜\"ë¼ê³  í•˜ëŠ” ê²ƒë³´ë‹¤, **\"ì½”ë“œ ì§œê³  ì‹¤í–‰í•´ë´. ì—ëŸ¬ ë‚˜ë©´ ê³ ì³ì„œ ë‹¤ì‹œ ì‹¤í–‰í•´ë´\"**ë¼ê³  ì‹œí‚¤ëŠ” ê²ƒì´ í›¨ì”¬ ê²°ê³¼ê°€ ì¢‹ìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ **Self-Correction(ìê°€ ìˆ˜ì •) Agent**ì˜ ê¸°ì´ˆì…ë‹ˆë‹¤.\n",
        "\n",
        "**ğŸ“š ê´€ë ¨ ì—°êµ¬:**\n",
        "> **Self-Refine: Iterative Refinement with Self-Feedback** (Madaan et al., 2023)  \n",
        "> [[Paper Link]](https://arxiv.org/abs/2303.17651)\n",
        "> *\"LLMì´ ì“´ ê¸€ì´ë‚˜ ì½”ë“œë¥¼ ìŠ¤ìŠ¤ë¡œ í”¼ë“œë°±ì„ ì£¼ê³  ê³ ì¹˜ê²Œ(Refine) í–ˆë”ë‹ˆ ì„±ëŠ¥ì´ í›¨ì”¬ ì¢‹ì•„ì§€ë”ë¼.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f75ed93",
        "outputId": "fb77ee65-dfc3-4dcd-896d-cbc72ec5511a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 1ì°¨ ìƒì„±ëœ ì½”ë“œ ###\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    a, b = 0, 1\n",
            "    for _ in range(2, n + 1):\n",
            "        a, b = b, a + b\n",
            "    return b\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. ê¸°ë³¸ì ì¸ ì½”ë“œ ìƒì„± í”„ë¡¬í”„íŠ¸\n",
        "code_gen_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert Python programmer. Return ONLY the code inside markdown code blocks. No explanation.\"),\n",
        "    (\"user\", \"{task}\")\n",
        "])\n",
        "\n",
        "code_chain = code_gen_prompt | code_llm | StrOutputParser()\n",
        "\n",
        "task = \"Write a Python function to calculate the N-th Fibonacci number efficiently.\"\n",
        "generated_code = code_chain.invoke({\"task\": task})\n",
        "print(\"### 1ì°¨ ìƒì„±ëœ ì½”ë“œ ###\")\n",
        "print(generated_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeac25c3"
      },
      "source": [
        "### ğŸ› ï¸ ì‹¤ìŠµ: ìê°€ ìˆ˜ì •(Self-Correction) ë£¨í”„ êµ¬í˜„\n",
        "ì•„ë˜ ì½”ë“œëŠ” AIê°€ ì§  ì½”ë“œë¥¼ ì‹¤ì œë¡œ ì‹¤í–‰í•´ë³´ê³ (`PythonREPL`), ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë‹¤ì‹œ AIì—ê²Œ ë³´ì—¬ì£¼ë©° \"ì´ê±° ì—ëŸ¬ ë‚¬ì–´. ê³ ì³ì¤˜\"ë¼ê³  ìš”ì²­í•˜ëŠ” ê³¼ì •ì„ ìë™í™”í•œ ê²ƒì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "125f5d93",
        "outputId": "1be8752a-38f5-4a2c-b212-bcc3efb93119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ì‹œë„ 1íšŒì°¨: ì‹¤í–‰ ì¤‘... ---/\n",
            "ğŸ‰ ì„±ê³µ! ì‹¤í–‰ ê²°ê³¼: Random Numbers: [84, 52, 7, 65, 68]\n",
            "Sorted Numbers: [7, 52, 65, 68, 84]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_experimental.utilities import PythonREPL\n",
        "import re\n",
        "\n",
        "repl = PythonREPL()\n",
        "\n",
        "def extract_code(text):\n",
        "    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ ìˆœìˆ˜ íŒŒì´ì¬ ì½”ë“œë§Œ ë°œë¼ë‚´ëŠ” í•¨ìˆ˜\n",
        "    pattern = r\"```python\\n(.*?)```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return text.replace(\"```\", \"\")\n",
        "\n",
        "def code_repair_loop(task, max_retries=3):\n",
        "    current_code = code_chain.invoke({\"task\": task})\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        clean_code = extract_code(current_code)\n",
        "        print(f\"\\n--- ì‹œë„ {i+1}íšŒì°¨: ì‹¤í–‰ ì¤‘... ---/\")\n",
        "        try:\n",
        "            result = repl.run(clean_code)\n",
        "\n",
        "            # ì—ëŸ¬ê°€ ì—†ëŠ”ì§€ í™•ì¸\n",
        "            if \"Error\" not in result and \"Traceback\" not in result:\n",
        "                print(\"ğŸ‰ ì„±ê³µ! ì‹¤í–‰ ê²°ê³¼:\", result)\n",
        "                return clean_code\n",
        "            else:\n",
        "                print(\"ğŸ’¥ ì‹¤í–‰ ì‹¤íŒ¨ (ì—ëŸ¬ ë°œìƒ):\", result)\n",
        "                # ğŸ’¡ í•µì‹¬: ì—ëŸ¬ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í”„ë¡¬í”„íŠ¸ì— ë„£ì–´ì„œ ìˆ˜ì •ì„ ìš”ì²­í•¨\n",
        "                fix_prompt = f\"The following python code failed with this error:\\n{result}\\n\\nCode:\\n{clean_code}\\n\\nFix the code and return only the full fixed code.\"\n",
        "                current_code = code_chain.invoke({\"task\": fix_prompt})\n",
        "        except Exception as e:\n",
        "            print(\"System Error:\", e)\n",
        "            break\n",
        "    return None\n",
        "\n",
        "complex_task = \"Write a python script that creates a list of 5 random numbers and prints their sorted values. Use the 'random' module.\"\n",
        "final_code = code_repair_loop(complex_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "184380e8"
      },
      "source": [
        "## 3. Advanced RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„± ì‹¬í™”)\n",
        "\n",
        "### ğŸ£ ê°œë… ì„¤ëª…: \"ì˜¤í”ˆë¶ ì‹œí—˜ì˜ ë‹¬ì¸\"\n",
        "**RAG**ëŠ” LLMì—ê²Œ 'êµê³¼ì„œ(ë¬¸ì„œ)'ë¥¼ ì¥ì–´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "í•˜ì§€ë§Œ ì§ˆë¬¸ì´ ì• ë§¤í•˜ë©´ êµê³¼ì„œ ì–´ë””ë¥¼ ë´ì•¼ í• ì§€ ëª¨ë¦…ë‹ˆë‹¤.\n",
        "\n",
        "- **Basic RAG**: \"ì´ê±° ì°¾ì•„ì¤˜\" -> (í‚¤ì›Œë“œ ë§¤ì¹­) -> \"ì—¬ê¸° ìˆìŠµë‹ˆë‹¤.\"\n",
        "- **Advanced RAG (MultiQuery)**: \"ì´ê±°ë‘ ë¹„ìŠ·í•œ ê±°, ê´€ë ¨ëœ ê±°, ë°˜ëŒ€ë˜ëŠ” ê±° ë‹¤ ì°¾ì•„ë´.\" -> ì§ˆë¬¸ì„ 3~4ê°œë¡œ ë»¥íŠ€ê¸°í•´ì„œ êµ¬ì„êµ¬ì„ ë‹¤ ì°¾ì•„ë´„.\n",
        "\n",
        "ì§ˆë¬¸ì„ ë‹¤ì–‘í•˜ê²Œ ë³€í˜•í•´ì„œ ê²€ìƒ‰í•˜ë©´, ë†“ì¹˜ëŠ” ì •ë³´ ì—†ì´ í›¨ì”¬ ì •í™•í•œ ë‹µì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "**ğŸ“š ê´€ë ¨ ì—°êµ¬:**\n",
        "> **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** (Lewis et al., 2020)  \n",
        "> [[Paper Link]](https://arxiv.org/abs/2005.11401)\n",
        "> *RAG ê°œë…ì„ ì²˜ìŒ ì •ë¦½í•œ ê¸°ë…ë¹„ì ì¸ ë…¼ë¬¸ì…ë‹ˆë‹¤.*\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png\" width=\"600\" alt=\"Agent Overview\">  \n",
        "*(ì´ë¯¸ì§€ ì¶œì²˜: [Lilian Weng Blog](https://lilianweng.github.io/posts/2023-06-23-agent/) - ììœ¨ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê°œìš”)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aa465f5",
        "outputId": "9517abee-a07b-4dde-a5aa-bf587b3b5c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¸ì„œ ìƒ‰ì¸ ì™„ë£Œ! ì´ì œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ (Lilian Wengì˜ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸)\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\n",
        "]\n",
        "loader = WebBaseLoader(urls)\n",
        "data = loader.load()\n",
        "\n",
        "# 2. ë¬¸ì„œ ìª¼ê°œê¸° (ë„ˆë¬´ ê¸¸ë©´ ëª» ì½ìœ¼ë‹ˆê¹Œ)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwuIkZPIcX_P",
        "outputId": "a763763c-a5fa-45d3-ad49-e72b4111c528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\"),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Component Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "retriever = TFIDFRetriever.from_documents(\n",
        "    splits\n",
        ")"
      ],
      "metadata": {
        "id": "wNLuT6HvcWFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the main components of an LLM-powered agent?\"\n",
        "docs = retriever.invoke(question)\n",
        "print(f\"\\nRetrieved {len(docs)} documents using TF-IDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkXIjBdrcb07",
        "outputId": "5fee1ee3-a158-4d4b-ae65-10834c108c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retrieved 4 documents using TF-IDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8xNVhlTclK1",
        "outputId": "72151eb1-6755-4956-c0b6-ab4f8516ee30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Component Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). â€œLLM-powered Autonomous Agentsâ€. Lilâ€™Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Nlp\\nLanguage-Model\\nAlignment\\nSteerability\\nPrompting\\n\\n\\n\\nÂ« \\n\\nLLM Powered Autonomous Agents\\n\\n\\n Â»\\n\\nThe Transformer Family Version 2.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nÂ© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\")]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë„ì„œê´€ ì±…ì¥)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "CCdi0gqxdDgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8ca625a",
        "outputId": "74b5013b-2926-4dff-b101-787c8564e97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Question: What are the main components of an LLM-powered agent?\n",
            "\n",
            "\n",
            "Retrieved 4 documents using MultiQuery.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.retrievers import TFIDFRetriever\n",
        "from langchain_core.documents import Document\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "# Test\n",
        "question = \"What are the main components of an LLM-powered agent?\"\n",
        "print(f\"Original Question: {question}\\n\")\n",
        "\n",
        "docs = vectorstore.similarity_search(question)\n",
        "print(f\"\\nRetrieved {len(docs)} documents using MultiQuery.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV-jjS9BdIN8",
        "outputId": "9dc694d5-b26e-4548-f9f7-81d31d0622c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='0969578a-1ff5-4d81-a8d6-ca9955e7dd3a', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
              " Document(id='1fce86fc-a1ce-44bc-9147-c05af34cbbfe', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
              " Document(id='5b765193-aba4-435c-9fb4-84050e2d513c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='Component Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning'),\n",
              " Document(id='db644349-9195-4f3c-9a83-77b898affb69', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Component Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b507cd1",
        "outputId": "770e414e-45de-406f-d633-c3fa303f6d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### AIì˜ ë‹µë³€ ###\n",
            "The main components of an LLM-powered agent are:\n",
            "\n",
            "1. Planning\n",
            "\t* Subgoal and decomposition: breaking down large tasks into smaller, manageable subgoals.\n",
            "\t* Reflection and refinement: self-criticism, self-reflection, learning from mistakes, and refining actions for future steps.\n",
            "2. Memory\n",
            "\t* Short-term memory: utilizing in-context learning to learn new information.\n",
            "\t* Long-term memory: retaining and recalling (infinite) information over extended periods using an external vector store and fast retrieval.\n",
            "3. Tool use: the agent learns to call external APIs for extra information that is missing from the model weights, including current information, code execution capability, access to proprietary information sources, etc.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG ì²´ì¸ ì¡°ë¦½\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "response = rag_chain.invoke(question)\n",
        "print(\"### AIì˜ ë‹µë³€ ###\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ac661ed"
      },
      "source": [
        "## 4. Tools & MCP (AIì—ê²Œ ì†ê³¼ ë°œ ë‹¬ì•„ì£¼ê¸°)\n",
        "\n",
        "### ğŸ£ ê°œë… ì„¤ëª…: \"ë‡Œë§Œ ìˆëŠ” ì²œì¬\" vs \"ì»´í“¨í„°ë¥¼ ì“°ëŠ” í‰ì¬\"\n",
        "LLMì€ ë‡Œë§Œ ë‘¥ë‘¥ ë– ìˆëŠ” ìƒíƒœì™€ ê°™ìŠµë‹ˆë‹¤. ê³„ì‚°ë„ ì˜ ëª»í•˜ê³ (4ìë¦¬ìˆ˜ ê³±ì…ˆ í‹€ë¦¼), ì˜¤ëŠ˜ ë‚ ì”¨ë„ ëª¨ë¦…ë‹ˆë‹¤.\n",
        "AIì—ê²Œ **ê³„ì‚°ê¸°(Tool)**ë‚˜ **ê²€ìƒ‰ì—”ì§„(Tool)**ì„ ì¥ì–´ì£¼ë©´ í›¨ì”¬ ë˜‘ë˜‘í•´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ëŸ°ë°, ê³„ì‚°ê¸° ì—°ê²°í•˜ëŠ” ì½”ë“œ ì§œê³ , ê²€ìƒ‰ì—”ì§„ ì—°ê²°í•˜ëŠ” ì½”ë“œ ì§œê³ ... ë„ˆë¬´ ê·€ì°®ì£ ?\n",
        "**MCP (Model Context Protocol)**ëŠ” **'AI ë„êµ¬ìš© USB í‘œì¤€'**ì…ë‹ˆë‹¤.\n",
        "ë§ˆìš°ìŠ¤ë‚˜ í‚¤ë³´ë“œë¥¼ USBì— ê½‚ìœ¼ë©´ ë°”ë¡œ ë™ì‘í•˜ë“¯ì´, MCP í‘œì¤€ì„ ë”°ë¥´ëŠ” ë„êµ¬ëŠ” ì–´ë–¤ AIì—ë„ ë°”ë¡œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "**ğŸ“š ê´€ë ¨ ì—°êµ¬:**\n",
        "> **Toolformer: Language Models Can Teach Themselves to Use Tools** (Schick et al., 2023)  \n",
        "> [[Paper Link]](https://arxiv.org/abs/2302.04761)\n",
        "> *\"ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë²•ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œ ë…¼ë¬¸\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1b544dd",
        "outputId": "00f8cd00-11ea-4e1e-eb97-6e96d59ad6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Calls: [{'name': 'calculate_length', 'args': {'text': 'Antigravity'}, 'id': 'c0f3a534-7868-46ad-9caf-ff7f15bf77c7', 'type': 'tool_call'}, {'name': 'reverse_string', 'args': {'text': 'Antigravity'}, 'id': 'e5af7f63-cc51-4404-886e-9e1c98fad053', 'type': 'tool_call'}]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "### 4.1 ê¸°ë³¸ì ì¸ ì»¤ìŠ¤í…€ ë„êµ¬ ì •ì˜ (@tool ë°ì½”ë ˆì´í„° ì‚¬ìš©)\n",
        "\n",
        "@tool\n",
        "def calculate_length(text: str) -> int:\n",
        "    \"\"\"Calculates the length of the given text string.\"\"\"\n",
        "    return len(text)\n",
        "\n",
        "@tool\n",
        "def reverse_string(text: str) -> str:\n",
        "    \"\"\"Reverses the given text string.\"\"\"\n",
        "    return text[::-1]\n",
        "\n",
        "tools = [calculate_length, reverse_string]\n",
        "\n",
        "# LLMì—ê²Œ ë„êµ¬ ì¥ì–´ì£¼ê¸° (bind_tools)\n",
        "# ì´ì œ LLMì€ í•„ìš”í•  ë•Œ ìŠ¤ìŠ¤ë¡œ ì´ íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê² ë‹¤ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "query = \"What is the length of 'Antigravity'? And also reverse it.\"\n",
        "ai_msg = llm_with_tools.invoke(query)\n",
        "\n",
        "# AIê°€ ì§ì ‘ ë‹µí•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, \"ì´ ë„êµ¬ ì¢€ ì¨ì£¼ì„¸ìš”\"ë¼ê³  ìš”ì²­(Calls)í•©ë‹ˆë‹¤.\n",
        "print(f\"Tool Calls: {ai_msg.tool_calls}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebc59dba"
      },
      "source": [
        "### 4.2 Model Context Protocol (MCP) ì‹¤ìŠµ\n",
        "ì´ì œ ë¡œì»¬ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¡œ **'ê°€ì§œ ì„œë²„'**ë¥¼ ë§Œë“¤ê³ , LangChain í´ë¼ì´ì–¸íŠ¸ë¡œ ì ‘ì†í•´ë´…ë‹ˆë‹¤.\n",
        "ì´ êµ¬ì¡°ëŠ” ë‚´ ì»´í“¨í„°ì˜ íŒŒì¼ ì‹œìŠ¤í…œì„ AIì—ê²Œ ì—´ì–´ì£¼ê±°ë‚˜, íšŒì‚¬ ë‚´ë¶€ DBë¥¼ AIì—ê²Œ ì—°ê²°í•  ë•Œ í‘œì¤€ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5f48c58",
        "outputId": "a6cf3836-c82c-4e23-8aff-bc1f7f0bd545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting simple_mcp_server.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile simple_mcp_server.py\n",
        "# 1. MCP ì„œë²„ ì½”ë“œ ì‘ì„± (ë³„ë„ íŒŒì¼ë¡œ ì €ì¥)\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "mcp = FastMCP(\"SimpleMath\")\n",
        "\n",
        "@mcp.tool()\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@mcp.tool()\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers\"\"\"\n",
        "    return a * b\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fde91c31"
      },
      "outputs": [],
      "source": [
        "# 2. MCP Clientë¡œ ìœ„ ì„œë²„ì— ì ‘ì†\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "\n",
        "# ì„œë²„ ì‹¤í–‰ íŒŒë¼ë¯¸í„°\n",
        "server_params = StdioServerParameters(\n",
        "    command=\"python\",\n",
        "    args=[\"simple_mcp_server.py\"],\n",
        ")\n",
        "\n",
        "async def use_mcp_tools():\n",
        "    # python í”„ë¡œì„¸ìŠ¤ë¡œ ì„œë²„ë¥¼ ë„ìš°ê³  í†µì‹ í•©ë‹ˆë‹¤.\n",
        "    async with stdio_client(server_params) as (read, write):\n",
        "        async with ClientSession(read, write) as session:\n",
        "            await session.initialize()\n",
        "\n",
        "            # ì„œë²„ê°€ ë¬´ìŠ¨ ë„êµ¬ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ ë¬¼ì–´ë´…ë‹ˆë‹¤.\n",
        "            tools = await session.list_tools()\n",
        "            print(f\"Connected to MCP Server. Available Tools: {[t.name for t in tools.tools]}\")\n",
        "\n",
        "            # ë„êµ¬ë¥¼ ì‚¬ìš©í•´ë´…ë‹ˆë‹¤.\n",
        "            result = await session.call_tool(\"add\", arguments={\"a\": 10, \"b\": 20})\n",
        "            print(f\"Result of 'add(10, 20)': {result.content}\")\n",
        "\n",
        "# await use_mcp_tools() # ì‹¤í–‰í•˜ë ¤ë©´ ì£¼ì„ í•´ì œ (Colab/JupyterëŠ” ì´ë¯¸ ë¹„ë™ê¸° í™˜ê²½ì´ë¼ await í•„ìš”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ba220a"
      },
      "source": [
        "## 5. Multi-Agent Systems & Skills (AI í˜‘ì—…)\n",
        "\n",
        "**\"ë¹¨ë¦¬ ê°€ë ¤ë©´ í˜¼ì ê°€ê³ , ë©€ë¦¬ ê°€ë ¤ë©´ í•¨ê»˜ ê°€ë¼\"**ëŠ” ë§ì´ ìˆì£ .\n",
        "ë³µì¡í•œ ë¬¸ì œëŠ” í˜¼ì í’€ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì—­í• ì„ ë‚˜ëˆ„ê³  í˜‘ì—…í•˜ëŠ” ë‹¤ì–‘í•œ **'ì—ì´ì „íŠ¸ ë””ìì¸ íŒ¨í„´'**ì„ ë°°ì›ë‹ˆë‹¤.\n",
        "\n",
        "### 5.1 ReAct (Reason + Act)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"ìƒê°í•˜ê³  ë§í•´!\"\n",
        "ê·¸ëƒ¥ ìƒê°ë‚˜ëŠ” ëŒ€ë¡œ ë§í•˜ëŠ”(ë‹µë³€í•˜ëŠ”) AIëŠ” ì‹¤ìˆ˜ë¥¼ ë§ì´ í•©ë‹ˆë‹¤.\n",
        "**ReAct**ëŠ” ë‹µë³€í•˜ê¸° ì „ì— ì†ìœ¼ë¡œ **Plan(ê³„íš)**ì„ ì§œê³ , **Act(ì‹¤í–‰)** í•´ë³´ê³ , **Observe(ê´€ì°°)**í•œ ë’¤ì— ìµœì¢… ë‹µë³€ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "1. **Thought**: \"ì‚¬ìš©ìê°€ 20 + 20ì„ ë¬¼ì–´ë´¤ë„¤. ê³„ì‚°ê¸° ë„êµ¬ë¥¼ ì¨ì•¼ê² ë‹¤.\"\n",
        "2. **Action**: `Calculator(20, 20)` í˜¸ì¶œ\n",
        "3. **Observation**: ê²°ê³¼ `40` í™•ì¸\n",
        "4. **Answer**: \"ì •ë‹µì€ 40ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "**ğŸ“š ê´€ë ¨ ì—°êµ¬:**\n",
        "> **ReAct: Synergizing Reasoning and Acting in Language Models** (Yao et al., 2022)\n",
        "> [[Paper Link]](https://arxiv.org/abs/2210.03629)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "0d28c8f7",
        "outputId": "0832e7e9-ac92-4e7c-84e5-801a1f5ac6a7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBTdGFydCAtLT4gVGhvdWdodAogICAgVGhvdWdodCAtLT587Ja0PyDrj4TqtazqsIAg7ZWE7JqU7ZWY64SkfCBBY3Rpb24KICAgIEFjdGlvbiAtLT4gVG9vbCgo64+E6rWsIOyCrOyaqSkpCiAgICBUb29sIC0tPiBPYnNlcnZhdGlvbgogICAgT2JzZXJ2YXRpb24gLS0+IFRob3VnaHQKICAgIFRob3VnaHQgLS0+fOydtOygnCDri7XsnYQg7JWM6rKg7Ja0fCBGaW5pc2gK\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "mm(\"\"\"\n",
        "graph TD\n",
        "    Start --> Thought\n",
        "    Thought -->|ì–´? ë„êµ¬ê°€ í•„ìš”í•˜ë„¤| Action\n",
        "    Action --> Tool((ë„êµ¬ ì‚¬ìš©))\n",
        "    Tool --> Observation\n",
        "    Observation --> Thought\n",
        "    Thought -->|ì´ì œ ë‹µì„ ì•Œê² ì–´| Finish\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab9c8279",
        "outputId": "70ded1ad-e9d0-48fc-8f76-e9cbc911d56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1316456668.py:5: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  react_agent = create_react_agent(llm, tools)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### ReAct ì‹¤í–‰ ê³¼ì • ###\n",
            "[human]: Calculate the length of 'Agentic' and then reverse the string 'Agentic'.\n",
            "[ai]: \n",
            "[tool]: 7\n",
            "[tool]: citnegA\n",
            "[ai]: The length of 'Agentic' is 7. The reverse of the string 'Agentic' is 'citnegA'.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# ReAct ì—ì´ì „íŠ¸ëŠ” LangGraphì—ì„œ ì•„ì£¼ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "tools = [calculate_length, reverse_string]\n",
        "react_agent = create_react_agent(llm, tools)\n",
        "\n",
        "query = \"Calculate the length of 'Agentic' and then reverse the string 'Agentic'.\"\n",
        "messages = react_agent.invoke({\"messages\": [(\"human\", query)]})\n",
        "\n",
        "print(\"### ReAct ì‹¤í–‰ ê³¼ì • ###\")\n",
        "for msg in messages['messages']:\n",
        "    print(f\"[{msg.type}]: {msg.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5482e14"
      },
      "source": [
        "### 5.2 Reflexion (Self-Reflection)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"ê¸€ì§“ê¸° í›„ í‡´ê³ í•˜ê¸°\"\n",
        "ì´ˆì•ˆì„ í•œ ë²ˆì— ì™„ë²½í•˜ê²Œ ì“°ëŠ” ì‚¬ëŒì€ ì—†ìŠµë‹ˆë‹¤.\n",
        "**Reflexion** íŒ¨í„´ì€ **ì‘ì„±(Generator)**í•˜ê³  **ë¹„í‰(Reflector)**í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
        "\"ë‚´ìš©ì´ ì¢€ ë¶€ì¡±í•œ ê±° ê°™ì€ë°?\", \"ì´ ë¶€ë¶„ì€ í‹€ë ¸ì–´\"ë¼ê³  ìŠ¤ìŠ¤ë¡œ í”¼ë“œë°±ì„ ì£¼ê³  ë‹¤ì‹œ ì”ë‹ˆë‹¤.\n",
        "\n",
        "**ğŸ“š ê´€ë ¨ ì—°êµ¬:**\n",
        "> **Reflexion: an autonomous agent with dynamic memory and self-reflection** (Shinn et al., 2023)\n",
        "> [[Paper Link]](https://arxiv.org/abs/2303.11366)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "4079d334",
        "outputId": "0b2c3e13-7f9d-48b1-e160-a541aed0c65a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSCiAgICBVc2VyIC0tPiBHZW5lcmF0b3IKICAgIEdlbmVyYXRvciAtLT587LSI7JWIIOyekeyEsXwgUmVmbGVjdG9yCiAgICBSZWZsZWN0b3IgLS0+fOu5hO2PiS/tlLzrk5zrsLF8IEdlbmVyYXRvcgogICAgUmVmbGVjdG9yIC0tPnzsmYTrsr3tlbQhfCBFbmQK\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "mm(\"\"\"\n",
        "graph LR\n",
        "    User --> Generator\n",
        "    Generator -->|ì´ˆì•ˆ ì‘ì„±| Reflector\n",
        "    Reflector -->|ë¹„í‰/í”¼ë“œë°±| Generator\n",
        "    Reflector -->|ì™„ë²½í•´!| End\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "020d7df1",
        "outputId": "22f5499f-3f1f-4005-a4cb-2e82f2bac124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reflexion Graph êµ¬ì„± ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import *\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, List, Annotated\n",
        "import operator\n",
        "\n",
        "# ìƒíƒœ ì •ì˜: ë©”ì‹œì§€ ëª©ë¡, í˜„ì¬ ì´ˆì•ˆ, ë¹„í‰ ë‚´ìš©, ë°˜ë³µ íšŸìˆ˜\n",
        "class ReflexionState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    draft: str\n",
        "    critique: str\n",
        "    revision_number: int\n",
        "\n",
        "def generator_node(state):\n",
        "    # ë¹„í‰ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ë°˜ì˜í•´ì„œ ë‹¤ì‹œ ì”ë‹ˆë‹¤.\n",
        "    if state.get(\"critique\"):\n",
        "        prompt = f\"Original Request: {state['messages'][0].content}\\nLast Draft: {state['draft']}\\nCritique: {state['critique']}\\n\\nPlease update the draft based on the critique.\"\n",
        "    else:\n",
        "        prompt = f\"Write a short essay about: {state['messages'][0].content}\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"draft\": response.content, \"revision_number\": state.get(\"revision_number\", 0) + 1}\n",
        "\n",
        "def reflector_node(state):\n",
        "    # ì´ˆì•ˆì„ ë³´ê³  ë¹„í‰í•©ë‹ˆë‹¤. ë„ˆê·¸ëŸ¬ìš´ ì„ ìƒë‹˜ ì—­í• ì…ë‹ˆë‹¤.\n",
        "    prompt = f\"Identify any missing points or errors in this draft: {state['draft']}. If it is good, say 'PERFECT'.\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"critique\": response.content}\n",
        "\n",
        "def should_continue(state):\n",
        "    # PERFECTë¥¼ ë°›ê±°ë‚˜ 3ë²ˆ ì´ìƒ ìˆ˜ì •í–ˆìœ¼ë©´ ëëƒ…ë‹ˆë‹¤.\n",
        "    if \"PERFECT\" in state[\"critique\"] or state[\"revision_number\"] > 2:\n",
        "        return END\n",
        "    return \"Generator\"\n",
        "\n",
        "workflow = StateGraph(ReflexionState)\n",
        "workflow.add_node(\"Generator\", generator_node)\n",
        "workflow.add_node(\"Reflector\", reflector_node)\n",
        "\n",
        "workflow.set_entry_point(\"Generator\")\n",
        "workflow.add_edge(\"Generator\", \"Reflector\")\n",
        "workflow.add_conditional_edges(\"Reflector\", should_continue)\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"Reflexion Graph êµ¬ì„± ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke({\"messages\": [HumanMessage(content=\"The future of AI\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAXuJuxqfVcq",
        "outputId": "ab950524-2930-4f11-9b8f-b477510b6951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})],\n",
              " 'draft': 'The simple yet profound greeting \"hi!\" It\\'s a phrase that has been used for centuries to acknowledge the presence of another person, to break the ice in social situations, and to express friendliness and approachability. But have you ever stopped to think about the significance of this seemingly innocuous word?\\n\\n\"Hi!\" is more than just a casual greeting; it\\'s a way of bridging the gap between individuals, creating a sense of connection and community. When we say \"hi!\" to someone, we\\'re not just acknowledging their physical presence, but also their humanity. We\\'re recognizing that they are another person with thoughts, feelings, and experiences, worthy of respect and consideration.\\n\\nIn many cultures, \"hi!\" is the first word spoken when meeting someone new, setting the tone for a positive interaction. It\\'s a way of showing enthusiasm and interest in getting to know the other person, rather than simply going through the motions of social etiquette. Whether it\\'s a quick hello on the street, a friendly wave at a party, or a warm welcome at a gathering, \"hi!\" has the power to break down barriers and create a sense of belonging.\\n\\nBut \"hi!\" is not just about social interactions; it\\'s also a reflection of our emotional state. When we say \"hi!\" with a smile and a spring in our step, we\\'re conveying confidence and positivity. We\\'re signaling that we\\'re approachable, friendly, and open to connection. On the other hand, when we say \"hi!\" with a hesitant tone or a forced smile, it can come across as insincere or even intimidating.\\n\\nIn conclusion, \"hi!\" may seem like a simple word, but its significance extends far beyond a casual greeting. It\\'s a powerful tool for building connections, creating community, and expressing ourselves in a way that\\'s authentic and approachable. So next time you say \"hi!\" to someone, remember the impact it can have â€“ not just on the other person, but also on yourself.',\n",
              " 'critique': 'The draft is well-written and thought-provoking. Here are some minor suggestions for improvement:\\n\\n* Consider adding a title to the piece to give it a clear focus and make it easier to scan.\\n* The transition between the first two paragraphs could be smoother. You might want to add a sentence or phrase to connect the ideas more explicitly.\\n* In the third paragraph, you mention that \"hi!\" is not just about social interactions, but also a reflection of our emotional state. This idea is interesting, but it\\'s not fully developed. You might want to elaborate on how our emotions influence our use of the word \"hi!\"\\n* The conclusion is a good summary of the main points, but it feels a bit abrupt. You could add a sentence or two to reinforce the significance of the word \"hi!\" and leave the reader with something to think about.\\n* Finally, consider adding some specific examples or anecdotes to illustrate the power of the word \"hi!\". This would make the piece more engaging and memorable.\\n\\nHere\\'s an edited version of the draft incorporating these suggestions:\\n\\n**The Power of a Simple Word: \"Hi!\"**\\n\\nThe simple yet profound greeting \"hi!\" has been used for centuries to acknowledge the presence of another person, break the ice in social situations, and express friendliness and approachability. But have you ever stopped to think about the significance of this seemingly innocuous word?\\n\\n\"Hi!\" is more than just a casual greeting; it\\'s a way of bridging the gap between individuals, creating a sense of connection and community. When we say \"hi!\" to someone, we\\'re not just acknowledging their physical presence, but also their humanity. We\\'re recognizing that they are another person with thoughts, feelings, and experiences, worthy of respect and consideration.\\n\\nIn many cultures, \"hi!\" is the first word spoken when meeting someone new, setting the tone for a positive interaction. It\\'s a way of showing enthusiasm and interest in getting to know the other person, rather than simply going through the motions of social etiquette. Whether it\\'s a quick hello on the street, a friendly wave at a party, or a warm welcome at a gathering, \"hi!\" has the power to break down barriers and create a sense of belonging.\\n\\nBut what\\'s interesting is that our emotional state can also influence how we use the word \"hi!\". When we\\'re feeling confident and positive, we tend to say \"hi!\" with a smile and a spring in our step. We\\'re signaling that we\\'re approachable, friendly, and open to connection. On the other hand, when we\\'re feeling anxious or uncertain, our tone may be hesitant or forced, which can come across as insincere or even intimidating.\\n\\nIn conclusion, \"hi!\" may seem like a simple word, but its significance extends far beyond a casual greeting. It\\'s a powerful tool for building connections, creating community, and expressing ourselves in a way that\\'s authentic and approachable. So next time you say \"hi!\" to someone, remember the impact it can have â€“ not just on the other person, but also on yourself. By using this simple word with intention and authenticity, we can create a ripple effect of kindness and connection that spreads far beyond our initial interaction.\\n\\n**PERFECT!**',\n",
              " 'revision_number': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "219225f1"
      },
      "source": [
        "### 5.3 Plan-and-Execute (ê³„íš í›„ ì‹¤í–‰)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"PMê³¼ ê°œë°œì\"\n",
        "- **Planner (PM)**: ì „ì²´ ë¡œë“œë§µì„ ì§­ë‹ˆë‹¤. \"1ë‹¨ê³„ëŠ” ê²€ìƒ‰, 2ë‹¨ê³„ëŠ” ë¶„ì„, 3ë‹¨ê³„ëŠ” ë³´ê³ ì„œ ì‘ì„±í•´.\"\n",
        "- **Executor (ì‹¤ë¬´ì)**: ê³„íší‘œë¥¼ ë³´ê³  í•˜ë‚˜ì”© ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "ReActëŠ” ëˆˆì•ì˜ ì¼ë§Œ ë³´ì§€ë§Œ, ì´ íŒ¨í„´ì€ **'í° ê·¸ë¦¼(Big Picture)'**ì„ ë³´ê³  ì›€ì§ì´ê¸° ë•Œë¬¸ì— ë³µì¡í•œ í”„ë¡œì íŠ¸ì— ì í•©í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ğŸ“š ê´€ë ¨ ì—°êµ¬:**\n",
        "> **Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models** (Wang et al., 2023)\n",
        "> [[Paper Link]](https://arxiv.org/abs/2305.04091)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40eaa798",
        "outputId": "50f941a7-e0dd-457e-d388-dbe189f150c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plan-and-Execute Graph êµ¬ì„± ì™„ë£Œ!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-648521108.py:12: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  \"parameters\": Plan.schema()\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.agents import AgentState # Recommended import path\n",
        "\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    steps: List[str] = Field(description=\"List of steps to follow\")\n",
        "\n",
        "# ê³„íšì„ êµ¬ì¡°ì ìœ¼ë¡œ ì§œê¸° ìœ„í•´ Tool Definition ì‚¬ìš©\n",
        "plan_function = {\n",
        "    \"name\": \"Plan\",\n",
        "    \"description\": \"Generate a step-by-step plan\",\n",
        "    \"parameters\": Plan.schema()\n",
        "}\n",
        "\n",
        "planning_llm = llm.bind_tools([plan_function])\n",
        "\n",
        "def plan_step(state: AgentState):\n",
        "    # PM: ê³„íšì„ ì„¸ì›€\n",
        "    return {\"messages\": [AIMessage(content=\"[Mock Plan] 1. Search for X. 2. Calculate Y.\")]}\n",
        "\n",
        "def execute_step(state: AgentState):\n",
        "    # Worker: ì‹œí‚¨ ëŒ€ë¡œ ì‹¤í–‰\n",
        "    return {\"messages\": [AIMessage(content=\"[Mock Execution] Executed Step 1 and 2.\")]}\n",
        "\n",
        "plan_exec_workflow = StateGraph(AgentState)\n",
        "plan_exec_workflow.add_node(\"Planner\", plan_step)\n",
        "plan_exec_workflow.add_node(\"Executor\", execute_step)\n",
        "plan_exec_workflow.set_entry_point(\"Planner\")\n",
        "plan_exec_workflow.add_edge(\"Planner\", \"Executor\")\n",
        "plan_exec_workflow.add_edge(\"Executor\", END)\n",
        "\n",
        "plan_exec_app = plan_exec_workflow.compile()\n",
        "print(\"Plan-and-Execute Graph êµ¬ì„± ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plan_exec_app.invoke({\"messages\": [(\"human\", \"What is the capital of France?\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vot-yPbf2BT",
        "outputId": "7cffd929-90db-45a6-c3d1-b89b4ed1c448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}, id='19e7a50c-e754-4a35-910d-ba3e10d1fe46'),\n",
              "  AIMessage(content='[Mock Plan] 1. Search for X. 2. Calculate Y.', additional_kwargs={}, response_metadata={}, id='6aceadc8-e3b5-4596-943c-a7ff2120dea3', tool_calls=[], invalid_tool_calls=[]),\n",
              "  AIMessage(content='[Mock Execution] Executed Step 1 and 2.', additional_kwargs={}, response_metadata={}, id='2aa78029-916d-486d-b988-4e984d3cabad', tool_calls=[], invalid_tool_calls=[])]}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7124366c"
      },
      "source": [
        "### 5.4 Parallelization (ë³‘ë ¬ ì²˜ë¦¬)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"ë¶„ì—…í™”\"\n",
        "í•œ ëª…ì´ ê²€ìƒ‰í•˜ê³ , ë¶„ì„í•˜ê³ , ê¸€ ì“°ê³  í•˜ë©´ í•˜ë£¨ ì¢…ì¼ ê±¸ë¦½ë‹ˆë‹¤.\n",
        "3ëª…ì´ ë™ì‹œì— ê°ì ë§¡ì€ ë¶€ë¶„ì„ ì¡°ì‚¬í•˜ê³ , ë‚˜ì¤‘ì— íŒ€ì¥ì´ ì·¨í•©(Aggregate)í•˜ë©´ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.\n",
        "ì´ê²ƒì´ ì†Œìœ„ ë§í•˜ëŠ” **Map-Reduce** êµ¬ì¡°ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "4a198d3e",
        "outputId": "0b747512-9d09-49a9-f24b-d87230afa12b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIFRECiAgICBTdGFydCAtLT4gRmFuT3V0CiAgICBGYW5PdXQgLS0+IEFnZW50MQogICAgRmFuT3V0IC0tPiBBZ2VudDIKICAgIEZhbk91dCAtLT4gQWdlbnQzCiAgICBBZ2VudDEgLS0+IEFnZ3JlZ2F0b3IKICAgIEFnZW50MiAtLT4gQWdncmVnYXRvcgogICAgQWdlbnQzIC0tPiBBZ2dyZWdhdG9yCiAgICBBZ2dyZWdhdG9yIC0tPiBFbmQK\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "mm(\"\"\"\n",
        "graph TD\n",
        "    Start --> FanOut\n",
        "    FanOut --> Agent1\n",
        "    FanOut --> Agent2\n",
        "    FanOut --> Agent3\n",
        "    Agent1 --> Aggregator\n",
        "    Agent2 --> Aggregator\n",
        "    Agent3 --> Aggregator\n",
        "    Aggregator --> End\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a89ad3",
        "outputId": "1c9fb326-0e54-4506-ea7b-f96f8c88baa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel Graph êµ¬ì„± ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "def analyst_1(state):\n",
        "    return {\"messages\": [AIMessage(content=\"[ë¶„ì„ê°€ 1] ì¬ë¬´ ê´€ì  ë¶„ì„ ê²°ê³¼...\")]}\n",
        "\n",
        "def analyst_2(state):\n",
        "    return {\"messages\": [AIMessage(content=\"[ë¶„ì„ê°€ 2] ê¸°ìˆ  ê´€ì  ë¶„ì„ ê²°ê³¼...\")]}\n",
        "\n",
        "def aggregator(state):\n",
        "    # ê° ë¶„ì„ê°€ë“¤ì˜ ì˜ê²¬ì„ í•˜ë‚˜ë¡œ ëª¨ìŒ\n",
        "    return {\"messages\": [AIMessage(content=\"[íŒ€ì¥] ì¢…í•© ì˜ê²¬: ë§¤ìˆ˜ ì¶”ì²œí•©ë‹ˆë‹¤.\")]}\n",
        "\n",
        "parallel_workflow = StateGraph(AgentState)\n",
        "parallel_workflow.add_node(\"Analyst1\", analyst_1)\n",
        "parallel_workflow.add_node(\"Analyst2\", analyst_2)\n",
        "parallel_workflow.add_node(\"Aggregator\", aggregator)\n",
        "\n",
        "# Fan-out: ì‹œì‘í•˜ìë§ˆì ë‘ ë¶„ì„ê°€ê°€ ë™ì‹œì— ì¶œë°œ\n",
        "parallel_workflow.add_edge(START, \"Analyst1\")\n",
        "parallel_workflow.add_edge(START, \"Analyst2\")\n",
        "\n",
        "# Fan-in: ë‘ ë¶„ì„ê°€ê°€ ëë‚˜ë©´ íŒ€ì¥ì—ê²Œ ë³´ê³ \n",
        "parallel_workflow.add_edge(\"Analyst1\", \"Aggregator\")\n",
        "parallel_workflow.add_edge(\"Analyst2\", \"Aggregator\")\n",
        "parallel_workflow.add_edge(\"Aggregator\", END)\n",
        "\n",
        "parallel_app = parallel_workflow.compile()\n",
        "print(\"Parallel Graph êµ¬ì„± ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_app.invoke({\"messages\": [(\"human\", \"ì¬ë¬´ ë¶„ì„ ê²°ê³¼...\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Zl-K1Mf5Og",
        "outputId": "d187ebb0-4fe5-4a6d-90bc-c8f133bba8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='ì¬ë¬´ ë¶„ì„ ê²°ê³¼...', additional_kwargs={}, response_metadata={}, id='17127da1-0b6b-4f1e-8309-241476e008fb'),\n",
              "  AIMessage(content='[ë¶„ì„ê°€ 1] ì¬ë¬´ ê´€ì  ë¶„ì„ ê²°ê³¼...', additional_kwargs={}, response_metadata={}, id='b4f4ba5b-44f8-4bae-8387-730bd00e687d', tool_calls=[], invalid_tool_calls=[]),\n",
              "  AIMessage(content='[ë¶„ì„ê°€ 2] ê¸°ìˆ  ê´€ì  ë¶„ì„ ê²°ê³¼...', additional_kwargs={}, response_metadata={}, id='5d1daadf-1eca-4bf0-b08d-5b788c699481', tool_calls=[], invalid_tool_calls=[]),\n",
              "  AIMessage(content='[íŒ€ì¥] ì¢…í•© ì˜ê²¬: ë§¤ìˆ˜ ì¶”ì²œí•©ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='84fdb197-f85f-47ea-827c-d0c267f6da59', tool_calls=[], invalid_tool_calls=[])]}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c1de43d"
      },
      "source": [
        "### 5.5 Supervisor Pattern (ê´€ë¦¬ì íŒ¨ë˜)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"íŒ€ì¥ë‹˜ì´ ì‘ì—… ë°°ë¶„\"\n",
        "ì‚¬ìš©ìê°€ ìš”ì²­ì„ í•˜ë©´, **íŒ€ì¥(Supervisor)**ì´ ëˆ„êµ¬í•œí…Œ ì‹œí‚¬ì§€ ê²°ì •í•©ë‹ˆë‹¤.\n",
        "- \"ìë£Œ ì¡°ì‚¬ê°€ í•„ìš”í•˜ë„¤? ê¹€ëŒ€ë¦¬(Researcher)ê°€ í•´.\"\n",
        "- \"ì´ì œ ì½”ë”©ì´ í•„ìš”í•˜ë„¤? ë°•ê³¼ì¥(Coder)ì´ í•´.\"\n",
        "\n",
        "Supervisor ìì²´ëŠ” ì¼ì„ ì•ˆ í•˜ê³  êµí†µì •ë¦¬(Routing)ë§Œ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb54f122"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "# 1. ìŠˆí¼ë°”ì´ì € ì—ì´ì „íŠ¸ ìƒì„±\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers:  [Researcher, Coder]. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When finished,\"\n",
        "    \" respond with FINISH.\"\n",
        ")\n",
        "\n",
        "options = [\"Researcher\", \"Coder\", \"FINISH\"]\n",
        "\n",
        "supervisor_chain = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"user\", \"{messages}\"),\n",
        "        (\"user\", f\"Given the conversation above, who should act next? Select one of: {options}\")\n",
        "    ])\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 2. ì›Œì»¤ ë…¸ë“œ ìƒì„± (ìƒëµ: ìœ„ì—ì„œ ë§Œë“  agent í™œìš©)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "supervisor_chain.invoke(input={\"messages\": \"What is the capital of France?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZyFQfoe1gNg7",
        "outputId": "c05c9acf-ab19-4a3f-8d75-08672f4af0c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Researcher should act next.\\n\\nThey will likely search for and retrieve information about the capital of France from a database or other source. \\n\\nStatus: The Coder is waiting for input to write code that can verify the answer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004a522f"
      },
      "source": [
        "## 6. Advanced Agentic Features (ì‹¤ì „ ê¸°ëŠ¥)\n",
        "\n",
        "ë°ëª¨ê°€ ì•„ë‹ˆë¼ **ì‹¤ì œ ì„œë¹„ìŠ¤**ë¥¼ ë§Œë“¤ë ¤ë©´ ê¼­ í•„ìš”í•œ 3ëŒ€ì¥ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "1. **Memory**: ëŒ€í™”ë¥¼ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "2. **Human-in-the-loop**: ìœ„í—˜í•œ ì¼ì€ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
        "3. **Evaluation**: ë‚´ ì—ì´ì „íŠ¸ê°€ ì–¼ë§ˆë‚˜ ë˜‘ë˜‘í•œì§€ ì ìˆ˜ë¥¼ ë§¤ê²¨ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29db5719"
      },
      "source": [
        "### 6.1 Memory (Persistence via Checkpointers)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"ê²Œì„ ì„¸ì´ë¸Œ í¬ì¸íŠ¸\"\n",
        "ê²Œì„ì„ í•˜ë‹¤ê°€ êº¼ë„, **ì„¸ì´ë¸Œ íŒŒì¼(Checkpoint)**ì´ ìˆìœ¼ë©´ ê·¸ ìë¦¬ì—ì„œ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆì£ ?\n",
        "LangGraphë„ Checkpointerë¥¼ ì“°ë©´, ëŒ€í™”ì˜ ëª¨ë“  ìƒíƒœë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "`thread_id`ë§Œ ì•Œë©´ ì–¸ì œë“  ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "670c3c3d",
        "outputId": "dc369d9c-0cbf-44fe-be6b-657d0f9186ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2076484534.py:7: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  persistent_agent = create_react_agent(llm, tools, checkpointer=memory)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ì²« ë²ˆì§¸ ëŒ€í™” ---\n",
            "The length of the string \"My name is Chulsoo\" is 13 characters.\n",
            "\n",
            "--- ë‘ ë²ˆì§¸ ëŒ€í™” (ìƒˆë¡œ ì‹¤í–‰í•´ë„ ê¸°ì–µí•¨) ---\n",
            "The length of the string \"What is my name?\" is 15 characters.\n",
            "\n",
            "To answer your original question, your name is Chulsoo.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# 1. ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ìƒì„± (ì‹¤ì œë¡œëŠ” Postgres ê°™ì€ DBë¥¼ ì”€)\n",
        "memory = MemorySaver()\n",
        "\n",
        "# 2. Agent ë§Œë“¤ ë•Œ memory ì „ë‹¬\n",
        "persistent_agent = create_react_agent(llm, tools, checkpointer=memory)\n",
        "\n",
        "# 3. thread_id ì„¤ì • (ì´ê²Œ ì‚¬ìš©ì IDë‚˜ ëŒ€í™”ë°© ID ì—­í• )\n",
        "thread_config = {\"configurable\": {\"thread_id\": \"chulsoo_chat_1\"}}\n",
        "\n",
        "# 4. ëŒ€í™” í…ŒìŠ¤íŠ¸\n",
        "print(\"--- ì²« ë²ˆì§¸ ëŒ€í™” ---\")\n",
        "msg1 = persistent_agent.invoke(\n",
        "    {\"messages\": [(\"human\", \"My name is Chulsoo.\")]},\n",
        "    config=thread_config\n",
        ")\n",
        "print(msg1['messages'][-1].content)\n",
        "\n",
        "print(\"\\n--- ë‘ ë²ˆì§¸ ëŒ€í™” (ìƒˆë¡œ ì‹¤í–‰í•´ë„ ê¸°ì–µí•¨) ---\")\n",
        "msg2 = persistent_agent.invoke(\n",
        "    {\"messages\": [(\"human\", \"What is my name?\")]},\n",
        "    config=thread_config\n",
        ")\n",
        "print(msg2['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35bead9"
      },
      "source": [
        "### 6.2 Human-in-the-loop (Interrupts)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"ë¶€ì¥ë‹˜ ê²°ì¬ ëŒ€ê¸°\"\n",
        "ì¸í„´(AI)ì´ ê±°ë˜ì²˜ì— ë©”ì¼ì„ ë³´ë‚´ë ¤ê³  í•©ë‹ˆë‹¤. ê·¸ëƒ¥ ë³´ë‚´ë©´ ì‚¬ê³ ì¹  ìˆ˜ ìˆì£ ?\n",
        "\"ë¶€ì¥ë‹˜(Human), ì´ê±° ë³´ë‚¼ê¹Œìš”?\" í•˜ê³  ë©ˆì¶°ì„œ í™•ì¸ë°›ëŠ” ê¸°ëŠ¥ì´ **`interrupt_before`** ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "099748d7",
        "outputId": "2b23344e-b927-4ed8-ac82-74769bbc2df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ì‹¤í–‰ ì‹œì‘ ---\n",
            "AI: íŒŒì¼ì„ ì „ë¶€ ì‚­ì œí•˜ë ¤ê³  í•©ë‹ˆë‹¤... (ì§„í–‰ ì „ ë©ˆì¶¤)\n",
            "\n",
            "--- (ì‚¬ìš©ì ê²€í†  ì¤‘) ì§„ì§œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ---\n"
          ]
        }
      ],
      "source": [
        "def step_1(state):\n",
        "    print(\"AI: íŒŒì¼ì„ ì „ë¶€ ì‚­ì œí•˜ë ¤ê³  í•©ë‹ˆë‹¤... (ì§„í–‰ ì „ ë©ˆì¶¤)\")\n",
        "    return {\"messages\": [AIMessage(content=\"Delete Plan Ready.\")]}\n",
        "\n",
        "def step_2_action(state):\n",
        "    print(\"AI: ì¾…! íŒŒì¼ ì‚­ì œ ì™„ë£Œ.\")\n",
        "    return {\"messages\": [AIMessage(content=\"Files deleted.\")]}\n",
        "\n",
        "workflow_hitl = StateGraph(AgentState)\n",
        "workflow_hitl.add_node(\"planner\", step_1)\n",
        "workflow_hitl.add_node(\"action\", step_2_action)\n",
        "\n",
        "workflow_hitl.set_entry_point(\"planner\")\n",
        "workflow_hitl.add_edge(\"planner\", \"action\")\n",
        "workflow_hitl.add_edge(\"action\", END)\n",
        "\n",
        "# ğŸ’¡ í•µì‹¬: 'action' ë…¸ë“œ ì‹¤í–‰ ì§ì „ì— ë©ˆì¶°ë¼!\n",
        "app_hitl = workflow_hitl.compile(checkpointer=memory, interrupt_before=[\"action\"])\n",
        "thread_config_hitl = {\"configurable\": {\"thread_id\": \"hitl_demo_1\"}}\n",
        "\n",
        "# ì‹¤í–‰\n",
        "print(\"--- ì‹¤í–‰ ì‹œì‘ ---\")\n",
        "app_hitl.invoke({\"messages\": [(\"human\", \"ì‚­ì œí•´.\")]}, config=thread_config_hitl)\n",
        "\n",
        "# ì§€ê¸ˆ ë©ˆì¶°ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ í™•ì¸ í›„ ìŠ¹ì¸í•´ì•¼ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n",
        "print(\"\\n--- (ì‚¬ìš©ì ê²€í†  ì¤‘) ì§„ì§œ í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ---\")\n",
        "# app_hitl.invoke(None, config=thread_config_hitl) # ì´ ì¤„ì„ ì‹¤í–‰í•´ì•¼ ë„˜ì–´ê°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87770af9"
      },
      "source": [
        "### 6.3 Evaluation with Ragas (ì„±ëŠ¥ í‰ê°€)\n",
        "\n",
        "#### ğŸ£ ë¹„ìœ : \"ì±„ì í‘œ\"\n",
        "ë‚´ê°€ ë§Œë“  AIê°€ ë©ì²­í•œì§€ ë˜‘ë˜‘í•œì§€ ëŠë‚Œìœ¼ë¡œë§Œ ì•Œë©´ ì•ˆë˜ê² ì£ ?\n",
        "**Ragas**ëŠ” ì‹œí—˜ ì¶œì œë¶€í„° ì±„ì ê¹Œì§€ ìë™ìœ¼ë¡œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
        "- **Faithfulness (ì‹ ë¢°ì„±)**: ì—†ëŠ” ì†Œë¦¬ ì§€ì–´ë‚´ì§€ ì•Šì•˜ë‚˜?\n",
        "- **Answer Relevancy (ê´€ë ¨ì„±)**: ë™ë¬¸ì„œë‹µ í•˜ì§€ ì•Šì•˜ë‚˜?\n",
        "ì´ëŸ° í•­ëª©ë“¤ì„ ìˆ˜ì¹˜ë¡œ ë”± ë³´ì—¬ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "0a1762400ba440c8b412fd8182364390",
            "6e63615cf22345d4984188d11b19e0f6",
            "96f2980b1fe5402fb55ea78277e0171e",
            "74459967060646f8ad9fda8803e68c00",
            "4609db42ebe244c3ae4ac870c98048ea",
            "0f4b51187c6d459188148527e98344af",
            "22da3843ae234819bfaf37b0fabe0b46",
            "d6ecb297437e4206b7962de970407e35",
            "353ab4c1ca624223a26ffa668191c010",
            "6d13dd48f28e4be489fd273211f4aa7f",
            "7004b748200d4b3b819b88c1853d3718"
          ]
        },
        "id": "40ea2c6a",
        "outputId": "aba55576-515f-4d3d-8229-88e47b425c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2444693692.py:3: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
            "  from ragas.metrics import faithfulness, answer_relevancy\n",
            "/tmp/ipython-input-2444693692.py:3: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
            "  from ragas.metrics import faithfulness, answer_relevancy\n",
            "/tmp/ipython-input-2444693692.py:9: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
            "  ragas_llm = LangchainLLMWrapper(llm)\n",
            "/tmp/ipython-input-2444693692.py:10: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
            "  ragas_embeddings = LangchainEmbeddingsWrapper(embedding)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ í‰ê°€ ë°ì´í„° ìƒì„± ì¤‘... ###\n",
            "Processing: What are the main components of an autonomous agent system?\n",
            "Processing: Explain the concept of 'Planning' in the context of LLM agents.\n",
            "\n",
            "### Ragas í‰ê°€ ì‹œì‘ (ë¡œì»¬ LLM ì‚¬ìš©) ###\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a1762400ba440c8b412fd8182364390"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### í‰ê°€ ê²°ê³¼ ###\n",
            "{'faithfulness': 0.2338, 'answer_relevancy': 0.8931}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the main components of an autonomous ...   \n",
              "1  Explain the concept of 'Planning' in the conte...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [Overview of a LLM-powered autonomous agent sy...   \n",
              "1  [Component Three: Tool Use\\n\\nCase Studies\\n\\n...   \n",
              "\n",
              "                                            response  \\\n",
              "0  According to the text, the main components of ...   \n",
              "1  In the context of LLM (Large Language Model) a...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0  The main components are Planning, Memory, and ...      0.181818   \n",
              "1  Planning usually involves breaking down a larg...      0.285714   \n",
              "\n",
              "   answer_relevancy  \n",
              "0          1.000000  \n",
              "1          0.786203  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f53ff0c7-778d-49f6-aae4-9f184f4e7af3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the main components of an autonomous ...</td>\n",
              "      <td>[Overview of a LLM-powered autonomous agent sy...</td>\n",
              "      <td>According to the text, the main components of ...</td>\n",
              "      <td>The main components are Planning, Memory, and ...</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Explain the concept of 'Planning' in the conte...</td>\n",
              "      <td>[Component Three: Tool Use\\n\\nCase Studies\\n\\n...</td>\n",
              "      <td>In the context of LLM (Large Language Model) a...</td>\n",
              "      <td>Planning usually involves breaking down a larg...</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.786203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f53ff0c7-778d-49f6-aae4-9f184f4e7af3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f53ff0c7-778d-49f6-aae4-9f184f4e7af3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f53ff0c7-778d-49f6-aae4-9f184f4e7af3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Explain the concept of 'Planning' in the context of LLM agents.\",\n          \"What are the main components of an autonomous agent system?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"In the context of LLM (Large Language Model) agents, \\\"Planning\\\" refers to the process by which an agent breaks down complex tasks into smaller, manageable subgoals. This enables efficient handling of complex tasks and allows the agent to plan ahead.\\n\\nThe planning component involves two key aspects:\\n\\n1. **Subgoal and decomposition**: The agent identifies the steps required to complete a task and decomposes it into smaller, more manageable subtasks.\\n2. **Reflection and refinement**: The agent reflects on its past actions, learns from mistakes, and refines its approach for future steps, thereby improving the quality of final results.\\n\\nBy planning ahead, LLM agents can effectively tackle complex tasks that would be difficult or impossible to accomplish through a single, straightforward action. This enables them to achieve their goals more efficiently and effectively.\",\n          \"According to the text, the main components of an autonomous agent system are:\\n\\n1. Planning\\n\\t* Subgoal and decomposition: breaking down large tasks into smaller, manageable subgoals.\\n\\t* Reflection and refinement: self-criticism, self-reflection, learning from mistakes, and refining future steps.\\n2. Memory\\n\\t* Short-term memory: utilizing in-context learning to learn.\\n\\t* Long-term memory: retaining and recalling information over extended periods using an external vector store and fast retrieval.\\n3. Tool use: the agent learns to call external APIs for extra information that is missing from the model weights, including current information, code execution capability, access to proprietary information sources, etc.\\n\\nThese components are mentioned in the text as part of the \\\"Agent System Overview\\\" section.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Planning usually involves breaking down a large task into smaller, manageable subgoals and reflecting on past actions to improve future results.\",\n          \"The main components are Planning, Memory, and Tool use.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07346563960379714,\n        \"min\": 0.18181818181818182,\n        \"max\": 0.2857142857142857,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2857142857142857,\n          0.18181818181818182\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15117699820293642,\n        \"min\": 0.7862034388225538,\n        \"max\": 0.9999999999999996,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7862034388225538,\n          0.9999999999999996\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "\n",
        "# 0. Ragas í‰ê°€ìš© LLM/Embedding ì„¤ì •\n",
        "# RagasëŠ” ìì²´ì ìœ¼ë¡œ APIë¥¼ í˜¸ì¶œí•˜ë ¤ í•˜ë¯€ë¡œ, ë¡œì»¬ LLMì„ ì“°ë ¤ë©´ Wrapperë¡œ ê°ì‹¸ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n",
        "ragas_llm = LangchainLLMWrapper(llm)\n",
        "ragas_embeddings = LangchainEmbeddingsWrapper(embedding)\n",
        "\n",
        "# 1. í‰ê°€í•  ì§ˆë¬¸ê³¼ ì •ë‹µ (Ground Truth) ì¤€ë¹„\n",
        "# ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "eval_questions = [\n",
        "    \"What are the main components of an autonomous agent system?\",\n",
        "    \"Explain the concept of 'Planning' in the context of LLM agents.\"\n",
        "]\n",
        "\n",
        "eval_ground_truths = [\n",
        "    \"The main components are Planning, Memory, and Tool use.\",\n",
        "    \"Planning usually involves breaking down a large task into smaller, manageable subgoals and reflecting on past actions to improve future results.\"\n",
        "]\n",
        "\n",
        "# 2. RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤ì œë¡œ ëŒë ¤ì„œ ë‹µë³€ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìˆ˜ì§‘\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "print(\"### ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ í‰ê°€ ë°ì´í„° ìƒì„± ì¤‘... ###\")\n",
        "\n",
        "for q in eval_questions:\n",
        "    print(f\"Processing: {q}\")\n",
        "\n",
        "    # 2.1 ê²€ìƒ‰ (Section 3ì—ì„œ ë§Œë“  retriever ì‚¬ìš©)\n",
        "    # Ragas í‰ê°€ë¥¼ ìœ„í•´ì„  ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©(contexts)ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "    docs = vectorstore.as_retriever().invoke(q)\n",
        "    context_text = [d.page_content for d in docs]\n",
        "    contexts.append(context_text)\n",
        "\n",
        "    # 2.2 ë‹µë³€ ìƒì„± (Section 3ì—ì„œ ë§Œë“  rag_chain ì‚¬ìš©)\n",
        "    ans = rag_chain.invoke(q)\n",
        "    answers.append(ans)\n",
        "\n",
        "# 3. ë°ì´í„°ì…‹ ìƒì„±\n",
        "data_samples = {\n",
        "    'question': eval_questions,\n",
        "    'answer': answers,\n",
        "    'contexts': contexts,\n",
        "    'ground_truth': eval_ground_truths\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "# 4. í‰ê°€ ì‹¤í–‰ (Evaluate)\n",
        "print(\"\\n### Ragas í‰ê°€ ì‹œì‘ (ë¡œì»¬ LLM ì‚¬ìš©) ###\")\n",
        "results = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[faithfulness, answer_relevancy],\n",
        "    llm=ragas_llm,\n",
        "    embeddings=ragas_embeddings\n",
        ")\n",
        "\n",
        "# 5. ê²°ê³¼ í™•ì¸\n",
        "print(\"\\n### í‰ê°€ ê²°ê³¼ ###\")\n",
        "print(results)\n",
        "\n",
        "# Pandas DataFrameìœ¼ë¡œ ë³´ê¸° í¸í•˜ê²Œ ë³€í™˜\n",
        "df_results = results.to_pandas()\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7368d13"
      },
      "source": [
        "## ğŸ‰ ë§ˆë¬´ë¦¬\n",
        "ì´ì œ ì—¬ëŸ¬ë¶„ì€ ë‹¨ìˆœí•œ LLM ì‚¬ìš©ìë¥¼ ë„˜ì–´, **'ì—ì´ì „íŠ¸ ì„¤ê³„ì(Agent Architect)'**ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "ì´ ë…¸íŠ¸ë¶ì— ìˆëŠ” ë¸”ë¡ë“¤ì„ ë ˆê³ ì²˜ëŸ¼ ì¡°ë¦½í•´ì„œ ìì‹ ë§Œì˜ ë©‹ì§„ AI ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Tn0ADnDZUlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a1762400ba440c8b412fd8182364390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e63615cf22345d4984188d11b19e0f6",
              "IPY_MODEL_96f2980b1fe5402fb55ea78277e0171e",
              "IPY_MODEL_74459967060646f8ad9fda8803e68c00"
            ],
            "layout": "IPY_MODEL_4609db42ebe244c3ae4ac870c98048ea"
          }
        },
        "6e63615cf22345d4984188d11b19e0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4b51187c6d459188148527e98344af",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_22da3843ae234819bfaf37b0fabe0b46",
            "value": "Evaluating:â€‡100%"
          }
        },
        "96f2980b1fe5402fb55ea78277e0171e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ecb297437e4206b7962de970407e35",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_353ab4c1ca624223a26ffa668191c010",
            "value": 4
          }
        },
        "74459967060646f8ad9fda8803e68c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d13dd48f28e4be489fd273211f4aa7f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7004b748200d4b3b819b88c1853d3718",
            "value": "â€‡4/4â€‡[01:14&lt;00:00,â€‡20.39s/it]"
          }
        },
        "4609db42ebe244c3ae4ac870c98048ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f4b51187c6d459188148527e98344af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22da3843ae234819bfaf37b0fabe0b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6ecb297437e4206b7962de970407e35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353ab4c1ca624223a26ffa668191c010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d13dd48f28e4be489fd273211f4aa7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7004b748200d4b3b819b88c1853d3718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}